{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-19T14:29:26.824624Z",
     "iopub.status.busy": "2021-10-19T14:29:26.823934Z",
     "iopub.status.idle": "2021-10-19T14:29:27.096117Z",
     "shell.execute_reply": "2021-10-19T14:29:27.095099Z",
     "shell.execute_reply.started": "2021-10-19T14:29:26.824585Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-10-19T14:29:27.372049Z",
     "iopub.status.busy": "2021-10-19T14:29:27.371673Z",
     "iopub.status.idle": "2021-10-19T14:29:38.981687Z",
     "shell.execute_reply": "2021-10-19T14:29:38.980598Z",
     "shell.execute_reply.started": "2021-10-19T14:29:27.372017Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frame_from_video(frame, video):\n",
    "    frame = frame - 1\n",
    "    !ffmpeg \\\n",
    "        -hide_banner \\\n",
    "        -loglevel fatal \\\n",
    "        -nostats \\\n",
    "        -i $video -vf \"select=eq(n\\,$frame)\" -vframes 1 frame.png\n",
    "    img = Image.open('frame.png')\n",
    "    os.remove('frame.png')\n",
    "    return img\n",
    "\n",
    "def annotate_frame(img, xc, yc, r, col = (57, 255, 20)):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for x, y in zip(xc, yc):\n",
    "        draw.ellipse((x-r, y-r, x+r, y+r), fill=col, outline='black')\n",
    "    return img\n",
    "\n",
    "# code from: https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\n",
    "def add_track_features(tracks, fps=59.94, snap_frame=10):\n",
    "    \"\"\"\n",
    "    Add column features helpful for syncing with video data.\n",
    "    \"\"\"\n",
    "    tracks = tracks.copy()\n",
    "    tracks[\"game_play\"] = (\n",
    "        tracks[\"gameKey\"].astype(\"str\")\n",
    "        + \"_\"\n",
    "        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n",
    "    )\n",
    "    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n",
    "    snap_dict = (\n",
    "        tracks.query('event == \"ball_snap\"')\n",
    "        .groupby(\"game_play\")[\"time\"]\n",
    "        .first()\n",
    "        .to_dict()\n",
    "    )\n",
    "    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n",
    "    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n",
    "    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n",
    "    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n",
    "        \"timedelta64[ms]\"\n",
    "    ) / 1_000\n",
    "    # Estimated video frame\n",
    "    tracks[\"est_frame\"] = (\n",
    "        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n",
    "    )\n",
    "    return tracks\n",
    "\n",
    "def add_video_features(videos):\n",
    "    videos['game_play'] = videos['video_frame'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
    "    videos['camera'] = videos['video_frame'].apply(lambda x: x.split('_')[2])\n",
    "    videos['frame'] = videos['video_frame'].apply(lambda x: x.split('_')[-1])\n",
    "    videos['xc'] = (videos['left'] + videos['width']/2).astype(int).values\n",
    "    videos['yc'] = (videos['top'] + videos['height']/2).astype(int).values\n",
    "    return videos\n",
    "\n",
    "\n",
    "def annotate_field(xc, yc, player, r = 10, width = 3, col = [(27, 3, 163), (255, 7, 58)], crop = None):\n",
    "    field = Image.open('../input/nflhelmet-helper-dataset/field.png')\n",
    "    w, h = field.size\n",
    "    zero = (68,68)\n",
    "    fs = (2424,1100)\n",
    "    draw = ImageDraw.Draw(field)\n",
    "    xc, yc = xc*fs[0]/120 + zero[0], (1 - yc/53.3)*fs[1] + zero[1]\n",
    "    for x, y, p in zip(xc, yc, player):\n",
    "        c = col[0] if p[0] == 'H' else col[1]\n",
    "        draw.ellipse((x-r, y-r, x+r, y+r), fill=c, width=width, outline = 'black')\n",
    "    if isinstance(crop, float):\n",
    "#         cp = [xc.min() - crop*w, yc.min() - crop*h, xc.max() + crop*w, yc.max() + crop*h]\n",
    "        cp = [xc.min() - crop*w, 0, xc.max() + crop*2*w, h]\n",
    "        return field.crop(cp)\n",
    "    else:\n",
    "        return field\n",
    "    \n",
    "    \n",
    "class show_play_with_tracking():\n",
    "    \n",
    "    def __init__(self, video_df = None, track_df = None):\n",
    "        if video_df is None:\n",
    "            video_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_baseline_helmets.csv')\n",
    "        self.video_df = add_video_features(video_df)\n",
    "        if track_df is None:\n",
    "            tracking_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_player_tracking.csv')\n",
    "            tracking_df = add_track_features(tracking_df)\n",
    "        self.tracking_df = tracking_df.query(\"est_frame > 0\")\n",
    "       \n",
    "    def __call__(self, game_play, frame, img_size = 800, video_folder = '../input/nfl-health-and-safety-helmet-assignment/train/'):\n",
    "        \n",
    "        camera = 'Sideline'\n",
    "        frame_side = get_frame_from_video(frame, video_folder + game_play + '_' + camera + '.mp4')\n",
    "        df = self.video_df.query(f\"game_play == '{game_play}' and frame == '{frame}' and camera == '{camera}'\")\n",
    "        frame_side = annotate_frame(frame_side, df.xc, df.yc, 10)\n",
    "\n",
    "        camera = 'Endzone'\n",
    "        frame_end = get_frame_from_video(frame, video_folder + game_play + '_' + camera + '.mp4')\n",
    "        df = self.video_df.query(f\"game_play == '{game_play}' and frame == '{frame}' and camera == '{camera}'\")\n",
    "        frame_end = annotate_frame(frame_end, df.xc, df.yc, 10)\n",
    "\n",
    "        frames = self.tracking_df['est_frame'].values\n",
    "        if frame not in frames:\n",
    "            index = np.absolute(frames-frame).argmin()\n",
    "            frame = frames[index]\n",
    "        df = self.tracking_df.query(f\"game_play == '{game_play}' and est_frame == {frame}\")\n",
    "        field = annotate_field(df.x, df.y, df.player, 20, crop = 0.01)\n",
    "\n",
    "\n",
    "        wf, hf = field.size\n",
    "        wc, hc = frame_side.size\n",
    "        field = field.resize((int(wf*2*hc/hf), 2*hc))\n",
    "        wf, hf = field.size\n",
    "\n",
    "        img = Image.new('RGB', (wf+wc+20, 2*hc+20))\n",
    "        img.paste(im=field, box=(5, 10))\n",
    "        img.paste(im=frame_side, box=(wf+15, 5))\n",
    "        img.paste(im=frame_end, box=(wf+15, hc+15))\n",
    "        img.thumbnail((img_size,img_size))\n",
    "        return img\n",
    "    \n",
    "spwt = show_play_with_tracking()\n",
    "\n",
    "# TODO, add interpolation of tracking_df and replace nearest\n",
    "class get_keypoints():\n",
    "    \n",
    "    def __init__(self, video_df = None, track_df = None):\n",
    "        if video_df is None:\n",
    "            video_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_baseline_helmets.csv')\n",
    "        self.video_df = add_video_features(video_df)\n",
    "        if track_df is None:\n",
    "            tracking_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_player_tracking.csv')\n",
    "            tracking_df = add_track_features(tracking_df)\n",
    "        self.tracking_df = tracking_df.query(\"est_frame > 0\")\n",
    "            \n",
    "    def __call__(self, game_play, frame, normalized = True):\n",
    "        keypoints = dict()\n",
    "        keypoints['Sideline'] = self.video_df.query(\n",
    "            f\"game_play == '{game_play}' and frame == '{frame}' and camera == 'Sideline'\")[['xc', 'yc']].values\n",
    "        keypoints['Endzone'] = self.video_df.query(\n",
    "            f\"game_play == '{game_play}' and frame == '{frame}' and camera == 'Endzone'\")[['xc', 'yc']].values\n",
    "        \n",
    "        frames = self.tracking_df['est_frame'].values\n",
    "        if frame not in frames:\n",
    "            index = np.absolute(frames-frame).argmin()\n",
    "            frame = frames[index]\n",
    "        keypoints['Tracking'] = self.tracking_df.query(\n",
    "            f\"game_play == '{game_play}' and est_frame == {frame}\")[['x', 'y']].values\n",
    "    \n",
    "        if normalized:\n",
    "            for k, v in keypoints.items():\n",
    "                keypoints[k] = (v - v.min(axis = 0)) / (v.max(axis = 0) - v.min(axis = 0))\n",
    "                \n",
    "        keypoints['Sideline'][:,1] = 1-keypoints['Sideline'][:,1]\n",
    "                \n",
    "        self.keypoints = keypoints\n",
    "            \n",
    "        return keypoints\n",
    "    \n",
    "    def plot(self, add_no = False):\n",
    "        if not hasattr(self, 'keypoints'):\n",
    "            print('you must run the function first...')\n",
    "        else:\n",
    "            kp = self.keypoints\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.scatter(kp['Endzone'][:,0], kp['Endzone'][:,1], marker = 'x', color = 'red', label = 'Endzone')\n",
    "            plt.scatter(kp['Sideline'][:,0], kp['Sideline'][:,1], marker = '^', color = 'red', label = 'Sideline')\n",
    "            plt.scatter(kp['Tracking'][:,0], kp['Tracking'][:,1], marker = 'o', color = 'green', label = 'Tracking')  \n",
    "            plt.legend();\n",
    "    \n",
    "get_kp = get_keypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-19T14:29:38.983547Z",
     "iopub.status.busy": "2021-10-19T14:29:38.983214Z",
     "iopub.status.idle": "2021-10-19T14:29:42.435653Z",
     "shell.execute_reply": "2021-10-19T14:29:42.434363Z",
     "shell.execute_reply.started": "2021-10-19T14:29:38.983515Z"
    }
   },
   "outputs": [],
   "source": [
    "spwt('57583_000082', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-19T14:29:42.438074Z",
     "iopub.status.busy": "2021-10-19T14:29:42.437784Z",
     "iopub.status.idle": "2021-10-19T14:29:42.460298Z",
     "shell.execute_reply": "2021-10-19T14:29:42.458443Z",
     "shell.execute_reply.started": "2021-10-19T14:29:42.438043Z"
    }
   },
   "outputs": [],
   "source": [
    "videoframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I wanted to do is to map each player from tracking data to the camera data like this:\n",
    "\n",
    "<img src=\"https://media.discordapp.net/attachments/874736660103962726/878082285667237898/unknown.png?width=911&height=702\" width=800 height=800 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same data can be normalized and viewed on the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.461712Z",
     "iopub.status.idle": "2021-10-19T14:29:42.46215Z"
    }
   },
   "outputs": [],
   "source": [
    "k = get_kp('57583_000082', 12)\n",
    "get_kp.plot(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a good portion of this comp comes down to point-cloud registration. There are plenty of methods that perform this taks. For example, OpenCV have `cv.findHomography`. As you can see below, this doesn't perform well. The main reason is that although the function uses Random sample consensus (RANSAC) to filter outliers, the clouds of points must be roughly alinged already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.464051Z",
     "iopub.status.idle": "2021-10-19T14:29:42.464899Z"
    }
   },
   "outputs": [],
   "source": [
    "srcPoints = k['Tracking'].astype('float32').reshape(-1,1,2)\n",
    "dstPoints = k['Sideline'].astype('float32').reshape(-1,1,2)\n",
    "M, mask = cv.findHomography(srcPoints, dstPoints, cv.RANSAC)\n",
    "print(mask.sum())\n",
    "tfmdPoints = cv.perspectiveTransform(srcPoints,M)\n",
    "plt.scatter(srcPoints[:,0,0], srcPoints[:,0,1], marker = 'o', color = 'red', label = 'source')\n",
    "plt.scatter(dstPoints[:,0,0], dstPoints[:,0,1], marker = '^', color = 'green', label = 'target')  \n",
    "plt.scatter(tfmdPoints[:,0,0], tfmdPoints[:,0,1], marker = 'o', color = 'blue', label = 'result')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Pytorch implementation (using gradient descent)\n",
    "\n",
    "So, isntead of using open CV, I decided to do this my won using pytorch. The idea here is to solve the folowing problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix} x^{'} \\\\ y^{'} \\\\ 1 \\end{bmatrix} = H \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} h_{11} & h_{12} & h_{13} \\\\ h_{21} & h_{22} & h_{23} \\\\ h_{31} & h_{32} & h_{33} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "where $H$ is a transformation matrix (that I want to find) and $(x', y')$ is the transformed coorinates of a point $(x, y)$.\n",
    "\n",
    "The optimization problem is to find $H$ such as $(x', y')$ is as close as possible to some ground truth $(x_t, y_t)$ cloud of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.468234Z",
     "iopub.status.idle": "2021-10-19T14:29:42.469072Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.470496Z",
     "iopub.status.idle": "2021-10-19T14:29:42.471147Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_mse(preds, targets):\n",
    "    d = torch.cdist(preds.squeeze(2), targets.squeeze(2))\n",
    "    loss = (d.min(dim = 1).values**2).mean().sqrt()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.47247Z",
     "iopub.status.idle": "2021-10-19T14:29:42.473109Z"
    }
   },
   "outputs": [],
   "source": [
    "def step(src, trg, m, lr = 3e-3, prt = True):\n",
    "    preds = torch.matmul(m, src) # Homography transform\n",
    "    loss = min_mse(preds, trg)   # mse between the closes pair of points\n",
    "    if prt: print(f'loss: {(loss.item()):.5f}')\n",
    "    loss.backward()\n",
    "    m.data -= lr * m.grad.data\n",
    "    m.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.474268Z",
     "iopub.status.idle": "2021-10-19T14:29:42.474914Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_predict(src, trg, init_rot = 0, lr = 3e-3, n_steps = 10000, verbose = True):\n",
    "    t = np.pi * init_rot / 180\n",
    "    m = torch.tensor([[np.cos(t),-np.sin(t), 0],\n",
    "                      [np.sin(t), np.cos(t), 0],\n",
    "                      [        0,         0, 1]], dtype = torch.double)\n",
    "    m.requires_grad_()\n",
    "    for i in range(n_steps): \n",
    "        if i % (n_steps//10) and verbose:\n",
    "            step(src, trg, m, lr=lr, prt=False)\n",
    "        else:\n",
    "            step(src, trg, m, lr=lr)\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        tfm = torch.matmul(m, src)\n",
    "        \n",
    "    if verbose:\n",
    "        plt.scatter(src[:,0], src[:,1], marker = 'o', color = 'red', label = 'source')\n",
    "        plt.scatter(trg[:,0], trg[:,1], marker = '^', color = 'green', label = 'target')  \n",
    "        plt.scatter(tfm[:,0], tfm[:,1], marker = 'o', color = 'blue', label = 'result')\n",
    "        plt.legend();\n",
    "        \n",
    "    return tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.476198Z",
     "iopub.status.idle": "2021-10-19T14:29:42.477404Z"
    }
   },
   "outputs": [],
   "source": [
    "k = get_kp('57583_000082', 1, True)\n",
    "src = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "trg = torch.cat([torch.tensor(k['Sideline']), torch.ones(len(k['Sideline'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "tfm = fit_predict(src, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.478442Z",
     "iopub.status.idle": "2021-10-19T14:29:42.479078Z"
    }
   },
   "outputs": [],
   "source": [
    "k = get_kp('57597_000658', 1, True)\n",
    "src = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "trg = torch.cat([torch.tensor(k['Sideline']), torch.ones(len(k['Sideline'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "tfm = fit_predict(src, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.480035Z",
     "iopub.status.idle": "2021-10-19T14:29:42.480521Z"
    }
   },
   "outputs": [],
   "source": [
    "k = get_kp('57781_000252', 1, True)\n",
    "src = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "trg = torch.cat([torch.tensor(k['Sideline']), torch.ones(len(k['Sideline'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "tfm = fit_predict(src, trg, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.481628Z",
     "iopub.status.idle": "2021-10-19T14:29:42.482234Z"
    }
   },
   "outputs": [],
   "source": [
    "spwt('57781_000252', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.483164Z",
     "iopub.status.idle": "2021-10-19T14:29:42.483797Z"
    }
   },
   "outputs": [],
   "source": [
    "spwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.48475Z",
     "iopub.status.idle": "2021-10-19T14:29:42.485372Z"
    }
   },
   "outputs": [],
   "source": [
    "video_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_baseline_helmets.csv')\n",
    "video_df = add_video_features(video_df)\n",
    "video_df.query(f\"game_play == '57781_000252' and frame == '1' and camera == 'Sideline'\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.486324Z",
     "iopub.status.idle": "2021-10-19T14:29:42.486921Z"
    }
   },
   "outputs": [],
   "source": [
    "video_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_baseline_helmets.csv')\n",
    "video_df = video_df.query('conf > 0.8')\n",
    "video_df = add_video_features(video_df)\n",
    "get_kp_highconf = get_keypoints(video_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.487842Z",
     "iopub.status.idle": "2021-10-19T14:29:42.488453Z"
    }
   },
   "outputs": [],
   "source": [
    "k = get_kp_highconf('57781_000252', 1, True)\n",
    "src = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "trg = torch.cat([torch.tensor(k['Sideline']), torch.ones(len(k['Sideline'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "tfm = fit_predict(src, trg, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.489368Z",
     "iopub.status.idle": "2021-10-19T14:29:42.489969Z"
    }
   },
   "outputs": [],
   "source": [
    "k = get_kp('57583_000082', 1, True)\n",
    "src = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "trg = torch.cat([torch.tensor(k['Endzone']), torch.ones(len(k['Endzone'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "print(src.shape, trg.shape)\n",
    "tfm = fit_predict(src, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T14:29:42.490893Z",
     "iopub.status.idle": "2021-10-19T14:29:42.491498Z"
    }
   },
   "outputs": [],
   "source": [
    "k = get_kp('57583_000082', 1, True)\n",
    "src = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "trg = torch.cat([torch.tensor(k['Endzone']), torch.ones(len(k['Endzone'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n",
    "print(src.shape, trg.shape)\n",
    "tfm = fit_predict(src, trg, 90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
