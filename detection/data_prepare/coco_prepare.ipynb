{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f72ecc-6f87-43ce-a45b-21b82ac29870",
   "metadata": {},
   "source": [
    "# single class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b2756-9a36-449d-912b-37eec6832b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T03:31:08.564372Z",
     "iopub.status.busy": "2021-09-24T03:31:08.563799Z",
     "iopub.status.idle": "2021-09-24T03:31:08.582052Z",
     "shell.execute_reply": "2021-09-24T03:31:08.581376Z",
     "shell.execute_reply.started": "2021-09-24T03:31:08.564128Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" \n",
    "    https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable\n",
    "    Special json encoder for numpy types\n",
    "    \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "class COCOConverter:\n",
    "    \"\"\"Class to convert competition csv to coco format.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame, \n",
    "        image_height: int = 720, \n",
    "        image_width: int = 1280, \n",
    "        type_agnostic: bool = False):\n",
    "        \n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.type_agnostic = type_agnostic\n",
    "        if self.type_agnostic:\n",
    "            self.categories = [{\"id\": 1, \"name\": \"Helmet\"}]\n",
    "        else:\n",
    "            self.categories = [\n",
    "                {\"id\": 1, \"name\": \"impact_None\",},\n",
    "                {\"id\": 2, \"name\": \"impact_Helmet\"},\n",
    "                {\"id\": 3, \"name\": \"impact_Shoulder\",},\n",
    "                {\"id\": 4, \"name\": \"impact_Body\"},\n",
    "                {\"id\": 5, \"name\": \"impact_Ground\",},\n",
    "                {\"id\": 6, \"name\": \"impact_Hand\"},\n",
    "            ]         \n",
    "        self.df = self._initialize(df)\n",
    "\n",
    "    def _get_file_name(self, row: pd.Series):\n",
    "        base_name = row.video[:-4]\n",
    "        file_name = f'{base_name}_frame{row.frame:04}.jpg'\n",
    "        return file_name\n",
    "\n",
    "    def _get_bbox(self, row: pd.Series):\n",
    "        return [row.left, row.top, row.width, row.height]\n",
    "\n",
    "    def _initialize(self, df: pd.DataFrame):\n",
    "        # set category id\n",
    "        if self.type_agnostic:\n",
    "            df['impactType'] = 'Helmet'\n",
    "            df['category_id'] = 1\n",
    "        else:\n",
    "            df['category_id'] = df['impactType'].map(\n",
    "                {\n",
    "                    'None': 1,\n",
    "                    'Helmet': 2,\n",
    "                    'Shoulder': 3,\n",
    "                    'Body': 4,\n",
    "                    'Ground': 5,\n",
    "                    'Hand': 6\n",
    "                }\n",
    "            )\n",
    "        # some preprocesses\n",
    "        df['file_name'] = df[['video', 'frame']].progress_apply(self._get_file_name, axis=1)\n",
    "        df['area'] = df['width'] * df['height']\n",
    "        df['bbox'] = df[['left', 'top', 'height', 'width']].progress_apply(self._get_bbox, axis=1)\n",
    "        df['iscrowd'] = 0\n",
    "        return df\n",
    "        \n",
    "\n",
    "    def save(self, save_path):\n",
    "        \"\"\"\n",
    "        Save as coco json format.\n",
    "        But also has many supplemental items like gameKey or view.\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "        image_df = df[['gameKey', 'playID', 'view', 'video', 'frame', 'file_name']].drop_duplicates()\n",
    "        image_df['height'] = self.image_height\n",
    "        image_df['width'] = self.image_width\n",
    "        \n",
    "        # add image id to images. Note that it's called just \"id\".\n",
    "        image_df['id'] = range(1, len(image_df) + 1)\n",
    "    \n",
    "        # add image id to annotations.\n",
    "        df['image_id'] = df[['file_name']].merge(image_df[['file_name', 'id']])['id'].values\n",
    "        df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "        print('start dumping...')\n",
    "        coco_annotations = dict()\n",
    "        coco_annotations['categories'] = self.categories\n",
    "        coco_annotations['images'] = [dict(row) for _, row in image_df.iterrows()]\n",
    "        coco_annotations['annotations'] = [dict(row) for _, row in df.iterrows()]\n",
    "        json.dump(coco_annotations, open(save_path, 'w'), indent=4, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a047de-c2bb-42fa-b0fd-ba2658863a68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T03:35:10.097756Z",
     "iopub.status.busy": "2021-09-24T03:35:10.097352Z",
     "iopub.status.idle": "2021-09-24T03:35:11.253682Z",
     "shell.execute_reply": "2021-09-24T03:35:11.252776Z",
     "shell.execute_reply.started": "2021-09-24T03:35:10.097709Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('work/NFL/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c3da09-2a42-4b19-8a63-8aaf8b6c44d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T03:35:27.291262Z",
     "iopub.status.busy": "2021-09-24T03:35:27.290876Z",
     "iopub.status.idle": "2021-09-24T03:35:27.309423Z",
     "shell.execute_reply": "2021-09-24T03:35:27.308669Z",
     "shell.execute_reply.started": "2021-09-24T03:35:27.291209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train videos: 48\n",
      "number of valid videos: 12\n"
     ]
    }
   ],
   "source": [
    "play_ids = df['playID'].unique()\n",
    "num_train = int(len(play_ids) * 0.8)\n",
    "train_play_ids = df['playID'].unique()[:num_train]\n",
    "valid_play_ids = df['playID'].unique()[num_train:]\n",
    "print('number of train videos:', len(train_play_ids))\n",
    "print('number of valid videos:', len(valid_play_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a762313b-4915-481a-a8a8-ddfc34ef6308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T03:35:45.663407Z",
     "iopub.status.busy": "2021-09-24T03:35:45.663029Z",
     "iopub.status.idle": "2021-09-24T03:35:45.908086Z",
     "shell.execute_reply": "2021-09-24T03:35:45.907252Z",
     "shell.execute_reply.started": "2021-09-24T03:35:45.663357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train annotations: 753170\n",
      "number of valid annotations: 198917\n"
     ]
    }
   ],
   "source": [
    "train_df = df.query('playID in @train_play_ids').reset_index(drop=True).copy()\n",
    "valid_df = df.query('playID in @valid_play_ids').reset_index(drop=True).copy()\n",
    "\n",
    "print('number of train annotations:', len(train_df))\n",
    "print('number of valid annotations:', len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1973644d-0d60-44b7-99ac-a6c568e91f24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T03:36:28.320332Z",
     "iopub.status.busy": "2021-09-24T03:36:28.319720Z",
     "iopub.status.idle": "2021-09-24T03:40:35.951471Z",
     "shell.execute_reply": "2021-09-24T03:40:35.950612Z",
     "shell.execute_reply.started": "2021-09-24T03:36:28.320044Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 753170/753170 [00:15<00:00, 49854.29it/s]\n",
      "100%|██████████| 753170/753170 [00:28<00:00, 26696.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start dumping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198917/198917 [00:03<00:00, 51147.13it/s]\n",
      "100%|██████████| 198917/198917 [00:07<00:00, 26484.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start dumping...\n"
     ]
    }
   ],
   "source": [
    "train_coco = COCOConverter(train_df, type_agnostic=True)\n",
    "train_coco.save('train_single.json')\n",
    "valid_coco = COCOConverter(valid_df, type_agnostic=True)\n",
    "valid_coco.save('valid_single.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cdfa9-3068-49c5-9c89-e34568b54d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T03:33:43.006158Z",
     "iopub.status.busy": "2021-09-24T03:33:43.005741Z",
     "iopub.status.idle": "2021-09-24T03:33:43.009558Z",
     "shell.execute_reply": "2021-09-24T03:33:43.008704Z",
     "shell.execute_reply.started": "2021-09-24T03:33:43.006105Z"
    }
   },
   "source": [
    "# multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "176e046b-fad0-45f4-8e73-33926e3abbb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T03:59:31.975543Z",
     "iopub.status.busy": "2021-09-29T03:59:31.975104Z",
     "iopub.status.idle": "2021-09-29T03:59:31.980927Z",
     "shell.execute_reply": "2021-09-29T03:59:31.980123Z",
     "shell.execute_reply.started": "2021-09-29T03:59:31.975475Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import Video, display\n",
    "import subprocess\n",
    "import gc\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a4a9e6-854f-4cdf-b0eb-076cb637b6aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T03:59:34.517335Z",
     "iopub.status.busy": "2021-09-29T03:59:34.516962Z",
     "iopub.status.idle": "2021-09-29T03:59:34.659720Z",
     "shell.execute_reply": "2021-09-29T03:59:34.658767Z",
     "shell.execute_reply.started": "2021-09-29T03:59:34.517275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ground truth bounding boxes:  193736\n",
      "Unique labels:  {'Helmet': 0, 'Helmet-Blurred': 1, 'Helmet-Difficult': 2, 'Helmet-Sideline': 3, 'Helmet-Partial': 4}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load image level csv file\n",
    "extra_df = pd.read_csv('./work/NFL/image_labels.csv')\n",
    "print('Number of ground truth bounding boxes: ', len(extra_df))\n",
    "\n",
    "# Number of unique labels\n",
    "label_to_id = {label: i for i, label in enumerate(extra_df.label.unique())}\n",
    "print('Unique labels: ', label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8000875a-bfb7-42ce-856e-c4851118a9d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T03:59:37.370497Z",
     "iopub.status.busy": "2021-09-29T03:59:37.370084Z",
     "iopub.status.idle": "2021-09-29T03:59:37.383160Z",
     "shell.execute_reply": "2021-09-29T03:59:37.382197Z",
     "shell.execute_reply.started": "2021-09-29T03:59:37.370430Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_ann_file(df, category_id):\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    data = dict(\n",
    "        info=dict(\n",
    "            description='NFL-Helmet-Assignment',\n",
    "            url=None,\n",
    "            version=None,\n",
    "            year=now.year,\n",
    "            contributor=None,\n",
    "            date_created=now.strftime('%Y-%m-%d %H:%M:%S.%f'),\n",
    "        ),\n",
    "        licenses=[dict(\n",
    "            url=None,\n",
    "            id=0,\n",
    "            name=None,\n",
    "        )],\n",
    "        images=[\n",
    "            # license, url, file_name, height, width, date_captured, id\n",
    "        ],\n",
    "        type='instances',\n",
    "        annotations=[\n",
    "            # segmentation, area, iscrowd, image_id, bbox, category_id, id\n",
    "        ],\n",
    "        categories=[\n",
    "            # supercategory, id, name\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    class_name_to_id = {}\n",
    "    labels =  [\"__ignore__\",\n",
    "                'Helmet',\n",
    "              'Helmet-Blurred', \n",
    "              'Helmet-Difficult', \n",
    "              'Helmet-Sideline',\n",
    "              'Helmet-Partial']\n",
    "\n",
    "    for i, each_label in enumerate(labels):\n",
    "        class_id = i - 1  # starts with -1\n",
    "        class_name = each_label\n",
    "        if class_id == -1:\n",
    "            assert class_name == '__ignore__'\n",
    "            continue\n",
    "        class_name_to_id[class_name] = class_id\n",
    "        data['categories'].append(dict(\n",
    "            supercategory=None,\n",
    "            id=class_id,\n",
    "            name=class_name,\n",
    "        ))\n",
    "    \n",
    "    box_id = 0\n",
    "    for i, image in enumerate(os.listdir(TRAIN_PATH)):\n",
    "\n",
    "        img = cv2.imread(TRAIN_PATH+'/'+image)\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        data['images'].append({\n",
    "            'license':0, \n",
    "            'url': None,\n",
    "            'file_name': image,\n",
    "            'height': height,\n",
    "            'width': width,\n",
    "            'date_camputured': None,\n",
    "            'id': i\n",
    "        })\n",
    "\n",
    "        df_temp = df[df.image == image]\n",
    "        for index, row in df_temp.iterrows():\n",
    "\n",
    "            area = round(row.width*row.height, 1)\n",
    "            bbox =[row.left, row.top, row.width, row.height]\n",
    "\n",
    "            data['annotations'].append({\n",
    "                'id': box_id,\n",
    "                'image_id': i,\n",
    "                'category_id': category_id[row.label],\n",
    "                'area': area,\n",
    "                'bbox':bbox,\n",
    "                'iscrowd':0\n",
    "            })\n",
    "            box_id+=1\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e8a3a1-0765-467e-a5ec-0592ded7fc67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T04:04:21.291452Z",
     "iopub.status.busy": "2021-09-29T04:04:21.291035Z",
     "iopub.status.idle": "2021-09-29T04:04:21.451306Z",
     "shell.execute_reply": "2021-09-29T04:04:21.450233Z",
     "shell.execute_reply.started": "2021-09-29T04:04:21.291387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (154988, 6) val: (38748, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_PATH = './work/NFL/images'\n",
    "extra_df = pd.read_csv('./work/NFL/image_labels.csv')\n",
    "\n",
    "category_id = {'Helmet':0, 'Helmet-Blurred':1,\n",
    "               'Helmet-Difficult':2, 'Helmet-Sideline':3,\n",
    "               'Helmet-Partial':4}\n",
    "\n",
    "df_train, df_val = train_test_split(extra_df, test_size=0.2, random_state=66)\n",
    "print('train:',df_train.shape,'val:',df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc2df7-19f7-4af9-9780-d7c64c0c79d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T04:05:06.643174Z",
     "iopub.status.busy": "2021-09-29T04:05:06.642913Z",
     "iopub.status.idle": "2021-09-29T04:05:07.422636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 9947,       training images: 7957,       validation images: 1990\n"
     ]
    }
   ],
   "source": [
    "image_bbox_label = {} \n",
    "for image, df in extra_df.groupby('image'): \n",
    "    image_bbox_label[image] = df.reset_index(drop=True)\n",
    "train_names, valid_names = train_test_split(list(image_bbox_label), test_size=0.2, random_state=42)\n",
    "print(f'Size of dataset: {len(image_bbox_label)},\\\n",
    "       training images: {len(train_names)},\\\n",
    "       validation images: {len(valid_names)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3947a2-e07c-458e-9041-5dacedbf9b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T04:05:07.423932Z",
     "iopub.status.busy": "2021-09-29T04:05:07.423689Z"
    }
   },
   "outputs": [],
   "source": [
    "frames=[]\n",
    "for i in train_names:\n",
    "    frames.append(extra_df[extra_df['image']==i])\n",
    "df_train = pd.concat(frames)\n",
    "\n",
    "\n",
    "frames=[]\n",
    "for i in valid_names:\n",
    "    frames.append(extra_df[extra_df['image']==i])\n",
    "df_val = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8140ed-c208-422f-ab50-bdccc57c3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790890d-9ac2-448a-8f37-9f6f026cc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extra_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755965e-f960-4ca0-8eb7-1f5b5d88a91b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ann_file_train = create_ann_file(df_train.reset_index(), category_id)\n",
    "ann_file_val = create_ann_file(df_val.reset_index(), category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631d173-1d80-4c0e-b4ce-91fb4b79c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train:',df_train.shape,'val:',df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df47183-545d-4c2d-91b5-4f5b24545922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as json to gdrive\n",
    "with open('work/NFL/ann_file_train.json', 'w') as f:\n",
    "    json.dump(ann_file_train, f, indent=4)\n",
    "        \n",
    "with open('work/NFL/ann_file_val.json', 'w') as f:\n",
    "    json.dump(ann_file_val, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b348ea92-5fd5-4857-ba25-fe3b521e03b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T15:50:58.035099Z",
     "iopub.status.busy": "2021-09-25T15:50:58.034691Z",
     "iopub.status.idle": "2021-09-25T15:50:58.039995Z",
     "shell.execute_reply": "2021-09-25T15:50:58.039179Z",
     "shell.execute_reply.started": "2021-09-25T15:50:58.035038Z"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import IPython\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from math import trunc\n",
    "from PIL import Image as PILImage\n",
    "from PIL import ImageDraw as PILImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b573017d-3cd6-45ee-b176-cbf1b201e429",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T15:51:00.916513Z",
     "iopub.status.busy": "2021-09-25T15:51:00.915957Z",
     "iopub.status.idle": "2021-09-25T15:51:00.959333Z",
     "shell.execute_reply": "2021-09-25T15:51:00.958418Z",
     "shell.execute_reply.started": "2021-09-25T15:51:00.916263Z"
    }
   },
   "outputs": [],
   "source": [
    "class CocoDataset():\n",
    "    def __init__(self, annotation_path, image_dir):\n",
    "        self.annotation_path = annotation_path\n",
    "        self.image_dir = image_dir\n",
    "        self.colors = ['blue', 'purple', 'red', 'green', 'orange', 'salmon', 'pink', 'gold',\n",
    "                        'orchid', 'slateblue', 'limegreen', 'seagreen', 'darkgreen', 'olive',\n",
    "                        'teal', 'aquamarine', 'steelblue', 'powderblue', 'dodgerblue', 'navy',\n",
    "                        'magenta', 'sienna', 'maroon']\n",
    "        \n",
    "        json_file = open(self.annotation_path)\n",
    "        self.coco = json.load(json_file)\n",
    "        json_file.close()\n",
    "        \n",
    "        #self.process_info()\n",
    "        #self.process_licenses()\n",
    "        self.process_categories()\n",
    "        self.process_images()\n",
    "        self.process_segmentations()\n",
    "\n",
    "    def display_info(self):\n",
    "        print('Dataset Info:')\n",
    "        print('=============')\n",
    "        for key, item in self.info.items():\n",
    "            print('  {}: {}'.format(key, item))\n",
    "        \n",
    "        requirements = [['description', str],\n",
    "                        ['url', str],\n",
    "                        ['version', str],\n",
    "                        ['year', int],\n",
    "                        ['contributor', str],\n",
    "                        ['date_created', str]]\n",
    "        for req, req_type in requirements:\n",
    "            if req not in self.info:\n",
    "                print('ERROR: {} is missing'.format(req))\n",
    "            elif type(self.info[req]) != req_type:\n",
    "                print('ERROR: {} should be type {}'.format(req, str(req_type)))\n",
    "        print('')\n",
    "        \n",
    "    def display_licenses(self):\n",
    "        print('Licenses:')\n",
    "        print('=========')\n",
    "        \n",
    "        requirements = [['id', int],\n",
    "                        ['url', str],\n",
    "                        ['name', str]]\n",
    "        for license in self.licenses:\n",
    "            for key, item in license.items():\n",
    "                print('  {}: {}'.format(key, item))\n",
    "            for req, req_type in requirements:\n",
    "                if req not in license:\n",
    "                    print('ERROR: {} is missing'.format(req))\n",
    "                elif type(license[req]) != req_type:\n",
    "                    print('ERROR: {} should be type {}'.format(req, str(req_type)))\n",
    "            print('')\n",
    "        print('')\n",
    "        \n",
    "    def display_categories(self):\n",
    "        print('Categories:')\n",
    "        print('=========')\n",
    "        for sc_key, sc_val in self.super_categories.items():\n",
    "            print('  super_category: {}'.format(sc_key))\n",
    "            for cat_id in sc_val:\n",
    "                print('    id {}: {}'.format(cat_id, self.categories[cat_id]['name']))\n",
    "            print('')\n",
    "    \n",
    "    def display_image(self, image_id, show_polys=False, show_bbox=True, show_labels=True, show_crowds=False, use_url=False):\n",
    "        print('Image:')\n",
    "        print('======')\n",
    "        if image_id == 'random':\n",
    "            image_id = random.choice(list(self.images.keys()))\n",
    "        \n",
    "        # Print the image info\n",
    "        image = self.images[image_id]\n",
    "        for key, val in image.items():\n",
    "            print('  {}: {}'.format(key, val))\n",
    "            \n",
    "        # Open the image\n",
    "        if use_url:\n",
    "            image_path = image['coco_url']\n",
    "            response = requests.get(image_path)\n",
    "            image = PILImage.open(BytesIO(response.content))\n",
    "            \n",
    "        else:\n",
    "            image_path = os.path.join(self.image_dir, image['file_name'])\n",
    "            image = PILImage.open(image_path)\n",
    "            \n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = \"data:image/png;base64, \" + base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "        # Calculate the size and adjusted display size\n",
    "        max_width = 900\n",
    "        image_width, image_height = image.size\n",
    "        adjusted_width = min(image_width, max_width)\n",
    "        adjusted_ratio = adjusted_width / image_width\n",
    "        adjusted_height = adjusted_ratio * image_height\n",
    "        \n",
    "        # Create list of polygons to be drawn\n",
    "        polygons = {}\n",
    "        bbox_polygons = {}\n",
    "        rle_regions = {}\n",
    "        poly_colors = {}\n",
    "        labels = {}\n",
    "        print('  segmentations ({}):'.format(len(self.segmentations[image_id])))\n",
    "        for i, segm in enumerate(self.segmentations[image_id]):\n",
    "            polygons_list = []\n",
    "            if segm['iscrowd'] != 0:\n",
    "                # Gotta decode the RLE\n",
    "                px = 0\n",
    "                x, y = 0, 0\n",
    "                rle_list = []\n",
    "                for j, counts in enumerate(segm['segmentation']['counts']):\n",
    "                    if j % 2 == 0:\n",
    "                        # Empty pixels\n",
    "                        px += counts\n",
    "                    else:\n",
    "                        # Need to draw on these pixels, since we are drawing in vector form,\n",
    "                        # we need to draw horizontal lines on the image\n",
    "                        x_start = trunc(trunc(px / image_height) * adjusted_ratio)\n",
    "                        y_start = trunc(px % image_height * adjusted_ratio)\n",
    "                        px += counts\n",
    "                        x_end = trunc(trunc(px / image_height) * adjusted_ratio)\n",
    "                        y_end = trunc(px % image_height * adjusted_ratio)\n",
    "                        if x_end == x_start:\n",
    "                            # This is only on one line\n",
    "                            rle_list.append({'x': x_start, 'y': y_start, 'width': 1 , 'height': (y_end - y_start)})\n",
    "                        if x_end > x_start:\n",
    "                            # This spans more than one line\n",
    "                            # Insert top line first\n",
    "                            rle_list.append({'x': x_start, 'y': y_start, 'width': 1, 'height': (image_height - y_start)})\n",
    "                            \n",
    "                            # Insert middle lines if needed\n",
    "                            lines_spanned = x_end - x_start + 1 # total number of lines spanned\n",
    "                            full_lines_to_insert = lines_spanned - 2\n",
    "                            if full_lines_to_insert > 0:\n",
    "                                full_lines_to_insert = trunc(full_lines_to_insert * adjusted_ratio)\n",
    "                                rle_list.append({'x': (x_start + 1), 'y': 0, 'width': full_lines_to_insert, 'height': image_height})\n",
    "                                \n",
    "                            # Insert bottom line\n",
    "                            rle_list.append({'x': x_end, 'y': 0, 'width': 1, 'height': y_end})\n",
    "                if len(rle_list) > 0:\n",
    "                    rle_regions[segm['id']] = rle_list  \n",
    "            # else:\n",
    "            #     # Add the polygon segmentation\n",
    "            #     for segmentation_points in segm['segmentation']:\n",
    "            #         segmentation_points = np.multiply(segmentation_points, adjusted_ratio).astype(int)\n",
    "            #         polygons_list.append(str(segmentation_points).lstrip('[').rstrip(']'))\n",
    "\n",
    "            polygons[segm['id']] = polygons_list\n",
    "\n",
    "            if i < len(self.colors):\n",
    "                poly_colors[segm['id']] = self.colors[i]\n",
    "            else:\n",
    "                poly_colors[segm['id']] = 'white'\n",
    "            \n",
    "            bbox = segm['bbox']\n",
    "            bbox_points = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1],\n",
    "                           bbox[0] + bbox[2], bbox[1] + bbox[3], bbox[0], bbox[1] + bbox[3],\n",
    "                           bbox[0], bbox[1]]\n",
    "            bbox_points = np.multiply(bbox_points, adjusted_ratio).astype(int)\n",
    "            bbox_polygons[segm['id']] = str(bbox_points).lstrip('[').rstrip(']')\n",
    "            \n",
    "            labels[segm['id']] = (self.categories[segm['category_id']]['name'], (bbox_points[0], bbox_points[1] - 4))\n",
    "            \n",
    "            # Print details\n",
    "            print('    {}:{}:{}'.format(segm['id'], poly_colors[segm['id']], self.categories[segm['category_id']]))\n",
    "\n",
    "        # Draw segmentation polygons on image\n",
    "        html = '<div class=\"container\" style=\"position:relative;\">'\n",
    "        html += '<img src=\"{}\" style=\"position:relative;top:0px;left:0px;width:{}px;\">'.format(img_str, adjusted_width)\n",
    "        html += '<div class=\"svgclass\"><svg width=\"{}\" height=\"{}\">'.format(adjusted_width, adjusted_height)\n",
    "        \n",
    "        if show_polys:\n",
    "            for seg_id, points_list in polygons.items():\n",
    "                fill_color = poly_colors[seg_id]\n",
    "                stroke_color = poly_colors[seg_id]\n",
    "                for points in points_list:\n",
    "                    html += '<polygon points=\"{}\" style=\"fill:{}; stroke:{}; stroke-width:1; fill-opacity:0.5\" />'.format(points, fill_color, stroke_color)\n",
    "        \n",
    "        if show_crowds:\n",
    "            for seg_id, rect_list in rle_regions.items():\n",
    "                fill_color = poly_colors[seg_id]\n",
    "                stroke_color = poly_colors[seg_id]\n",
    "                for rect_def in rect_list:\n",
    "                    x, y = rect_def['x'], rect_def['y']\n",
    "                    w, h = rect_def['width'], rect_def['height']\n",
    "                    html += '<rect x=\"{}\" y=\"{}\" width=\"{}\" height=\"{}\" style=\"fill:{}; stroke:{}; stroke-width:1; fill-opacity:0.5; stroke-opacity:0.5\" />'.format(x, y, w, h, fill_color, stroke_color)\n",
    "            \n",
    "        if show_bbox:\n",
    "            for seg_id, points in bbox_polygons.items():\n",
    "                fill_color = poly_colors[seg_id]\n",
    "                stroke_color = poly_colors[seg_id]\n",
    "                html += '<polygon points=\"{}\" style=\"fill:{}; stroke:{}; stroke-width:1; fill-opacity:0\" />'.format(points, fill_color, stroke_color)\n",
    "                \n",
    "        if show_labels:\n",
    "            for seg_id, label in labels.items():\n",
    "                color = poly_colors[seg_id]\n",
    "                html += '<text x=\"{}\" y=\"{}\" style=\"fill:{}; font-size: 12pt;\">{}</text>'.format(label[1][0], label[1][1], color, label[0])\n",
    "                \n",
    "        html += '</svg></div>'\n",
    "        html += '</div>'\n",
    "        html += '<style>'\n",
    "        html += '.svgclass { position:absolute; top:0px; left:0px;}'\n",
    "        html += '</style>'\n",
    "        return html\n",
    "       \n",
    "    def process_info(self):\n",
    "        self.info = self.coco['info']\n",
    "    \n",
    "    def process_licenses(self):\n",
    "        self.licenses = self.coco['licenses']\n",
    "    \n",
    "    def process_categories(self):\n",
    "        self.categories = {}\n",
    "        self.super_categories = {}\n",
    "        for category in self.coco['categories']:\n",
    "            cat_id = category['id']\n",
    "            super_category = category['supercategory']\n",
    "            \n",
    "            # Add category to the categories dict\n",
    "            if cat_id not in self.categories:\n",
    "                self.categories[cat_id] = category\n",
    "            else:\n",
    "                print(\"ERROR: Skipping duplicate category id: {}\".format(category))\n",
    "\n",
    "            # Add category to super_categories dict\n",
    "            if super_category not in self.super_categories:\n",
    "                self.super_categories[super_category] = {cat_id} # Create a new set with the category id\n",
    "            else:\n",
    "                self.super_categories[super_category] |= {cat_id} # Add category id to the set\n",
    "                \n",
    "    def process_images(self):\n",
    "        self.images = {}\n",
    "        for image in self.coco['images']:\n",
    "            image_id = image['id']\n",
    "            if image_id in self.images:\n",
    "                print(\"ERROR: Skipping duplicate image id: {}\".format(image))\n",
    "            else:\n",
    "                self.images[image_id] = image\n",
    "                \n",
    "    def process_segmentations(self):\n",
    "        self.segmentations = {}\n",
    "        for segmentation in self.coco['annotations']:\n",
    "            image_id = segmentation['image_id']\n",
    "            if image_id not in self.segmentations:\n",
    "                self.segmentations[image_id] = []\n",
    "            self.segmentations[image_id].append(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6e04983-42c9-4916-bd96-eee1a6c1020c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T15:37:06.969853Z",
     "iopub.status.busy": "2021-09-25T15:37:06.969252Z",
     "iopub.status.idle": "2021-09-25T15:37:06.999858Z",
     "shell.execute_reply": "2021-09-25T15:37:06.999012Z",
     "shell.execute_reply.started": "2021-09-25T15:37:06.969571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories:\n",
      "=========\n",
      "  super_category: None\n",
      "    id 0: Helmet\n",
      "    id 1: Helmet-Blurred\n",
      "    id 2: Helmet-Difficult\n",
      "    id 3: Helmet-Sideline\n",
      "    id 4: Helmet-Partial\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotation_path = r'work/NFL/ann_file_train.json'\n",
    "image_dir = r'work/NFL/images'\n",
    "\n",
    "coco_dataset = CocoDataset(annotation_path, image_dir)\n",
    "# coco_dataset.display_info()\n",
    "# coco_dataset.display_licenses()\n",
    "coco_dataset.display_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "552c536b-3aa6-4fc9-9139-bf167397c916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-25T15:37:10.420812Z",
     "iopub.status.busy": "2021-09-25T15:37:10.420195Z",
     "iopub.status.idle": "2021-09-25T15:37:10.876934Z",
     "shell.execute_reply": "2021-09-25T15:37:10.875820Z",
     "shell.execute_reply.started": "2021-09-25T15:37:10.420530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:\n",
      "======\n",
      "  license: 0\n",
      "  url: None\n",
      "  file_name: 57873_000399_Endzone_frame1010.jpg\n",
      "  height: 720\n",
      "  width: 1280\n",
      "  date_camputured: None\n",
      "  id: 2066\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2066",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-8a558e72f7fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-1dec7889fbdf>\u001b[0m in \u001b[0;36mdisplay_image\u001b[0;34m(self, image_id, show_polys, show_bbox, show_labels, show_crowds, use_url)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mpoly_colors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  segmentations ({}):'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mpolygons_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2066"
     ]
    }
   ],
   "source": [
    "html = coco_dataset.display_image('random', use_url=False)\n",
    "IPython.display.HTML(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
